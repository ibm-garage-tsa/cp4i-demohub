{"componentChunkName":"component---src-pages-tutorials-create-engaging-experiences-old-index-mdx","path":"/tutorials/CreateEngagingExperiences-old/","result":{"pageContext":{"frontmatter":{"title":"Augment existing business functions with new applications using Kafka"},"relativePagePath":"/tutorials/CreateEngagingExperiences-old/index.mdx","titleType":"page","MdxNode":{"id":"1b412250-7163-53e7-8fab-f78d562a83fe","children":[],"parent":"c89517b8-4185-5a09-b5cb-fafc1c383ac1","internal":{"content":"---\ntitle: Augment existing business functions with new applications using Kafka\n---\n\nimport ArticleDetails from '../../../../src/gatsby-theme-carbon/components/ArticleDetails'\n\n<ArticleDetails name=\"Carlos Hirata, Ravi Katikala\" lastUpdated=\"January 2021\" readTimeMinutes=\"60\" />\n\nIn this tutorial, you use the Cloud Pak for Integration 2020.4.1 to create an App Integration flow (IBM App Connect Enterprise) that sends messages to a queue (IBM MQ) on OpenShift 4.6.\n\n<AnchorLinks>\n  <AnchorLink>Event Streams Topic</AnchorLink>\n  <AnchorLink>Configuring App Connect Enterprise</AnchorLink>\n  <AnchorLink>Deploying App Connect Enterprise</AnchorLink>\n  <AnchorLink>Testing App Connect Enterprise</AnchorLink>\n  <AnchorLink>Operations Dashboard (tracing)</AnchorLink>\n  <AnchorLink>Summary</AnchorLink>\n</AnchorLinks>\n\n### Introduction\n\nAugment existing business functions with new applications using Kafka.\n\nThe most interesting and impactful new applications in an enterprise are those that provide interactive experiences by reacting to existing systems carrying out a business function. In this tutorial, we take a look at an example from the retail industry. Starting with an existing API orchestrating the business function to \"place an order\". Let's say that when a customer places an order, we want to provide a real-time response. We want to reward the customer with points in a customer loyalty app or gamification experience or sign them up for a certain email nurture program. To do that, we need each order to emit an event. The Cloud Pak for Integration combines integration capabilities with Kafka based event streaming to make the data available for cloud-native applications to subscribe to a Kafka topic and use it for various business purposes.\n\nIn this tutorial, you create a topic in Kafka, modify an integration flow to call an API, emit an event onto the topic, and use the tracing tool to verify the message from App Connect Enterprise to Event Streams.\nThe Cloud Pak for Integration - Operations Dashboard Add-on provides cross-component monitoring and tracing capabilities that enable IT personnel to quickly analyze performance bottlenecks and latency problems across integration components to help ensure high levels of service.\n\n### Takeaways\n\n- Creating and Configuring an Event Streams Topic\n- Configure App Connect Enterprise message flow using App Connect Enterprise toolkit\n- Configuring App Connect Enterprise service\n- Deploying App Connect BAR file on App Connect Enterprise Server\n- Testing App Connect Enterprise API sending a message to Event Streams\n- Checking this message using Operations Dashboard (tracing).\n\n\n\n## Event Streams Topic\n\n### Task 1 – Creating and Configuring an Event Streams Topic\n\nCreating an Event Streams topic in the existing Event Streams instance.\n\n1.Before you start the lab. Download **eslab.zip** from **Github**, open a browser and enter: https://github.com/ibm-cloudintegration/dte-labs/tree/master/Lab3-RespondtoEvents . Click **eslab.zip** .\n\n![](images/lab3-task2.1-eslab.png)\n\n2.Click **Download**. The file **eslab.zip** is in your  **~/Download** directory.\n\n![](images/lab3-task2.2-download-eslab.png)\n\n3.Execute **unzip eslab.zip** will create the work directory **~/eslab** with 3 files: **ace-es.zip** (App Connect Enterprise project), **setdbparms.txt** (App Connect configuration file) and **bootstrap.address** (you can use this file to store Event Streams Bootstrap Address).\n\n![](images/lab3-task2.3-unzip-eslab.png)\n\n4.In **Runtimes** click **event-streams** link (in the kafka cluster instance type).\n\n![](images/lab3-task2.4-runtimes-eventstreams.png)\n\n5.In the Welcome to IBM Event Streams page. Click **System is healthy**  and check the status of the instance.\n\n![](images/lab3-task2.5-access-system-healthy.png)\n\n6.Verify if all Event Streams components are running and ready.\n\n![](images/lab3-task2.6-systemishealthy.png)\n\n7.Click **Create a topic** to configure topic.\n\n![](images/lab3-task2.7-createatopic.png)\n\n8.View the full range of configuration options by setting the Show all available options to on. Create the topic as follows:\n\n```\n1.In Show all available options, change to On.\n2.Topic name: customerinfo.\n3.Click Create topic.\n\n```\n\n![](images/lab3-task2.8-createatopic-parameters.png)\n\n9.The topic is created, click to **connect to this cluster**. By default Event Streams requires clients to be authorized to write to topics. The available authentication mechanisms for use with the REST Producer are MutualTLS (tls) and SCRAM SHA 512 (scram-sha-512). For more information about these authentication mechanisms (https://en.wikipedia.org/wiki/Salted_Challenge_Response_Authentication_Mechanism).\n\n![](images/lab3-task2.9-connecttothiscluster.png)\n\n10.To connect an application or tool to this cluster, you need the address of a Kafka listener, a credential to authenticate with, and a certificate depending on the listener.\nFor this lab, use an External listener. App Connect needs securities parameters to access Event Streams, the truststore password, certificate and SCRAM user ID and password.\nSave the bootstrap server **External (1)**, clicking the **copy icon**. You can use **bootstrap.address** file.\nFor this lab you use external listener to verify how to access Event Streams as external application.\n\n![](images/lab3-task2.10-savebootstrap.png)\n\n11.Click **Generate SCRAM credentials** .\n\n![](images/lab3-task2.11-generatescram.png)\n\n12.The Generate credentials for your application window you create credential name and how your application works with Event Streams. Credential name, it is your kafkauser. You can see this in the OpenShift console.\n\n```\n1.\tEnter the credential name: customerinfoapp.\n2.\tCheck Produce and consume messages, and read schemas.\n3.\tClick Next.\n\n```\n![](images/lab3-task2.12-generate-credentials.png)\n\n13.In the window, Which topics does the application need to access?, you can specify which the topics, the application accesses.\n\n```\n1.\tCheck All topics.\n2.\tClick Next.\n\n```\n![](images/lab3-task2.13-which-consumer-access.png)\n\n14.In the window, **Which consumer group does the application need to access?**, a consumer group is a group of consumers cooperating to consume messages from one or more topics. The consumers in a group all use the same value for the group ID configuration.\n\n```\n1.\tCheck All consumer groups.\n2.\tClick Next.\n\n```\n\n![](images/lab3-task2.14-which-topics-need-access.png)\n\n15.In the window, **Choose which transactional IDs the application can access?**, you can control the ability to use the transaction capability in Kafka.\n\n```\n1.Keep No transactional IDs checked.\n2.Click Generate credentials.\n\n```\n![](images/lab3-task2.15-which-transactional-id.png)\n\n16.Back to **Cluster connection** window. Verify the bootstrap server as an external listener, the username and password in **Generated Scram Credentials**. Scroll to **Certificates** and click **Download certificate**. Save **es-cert.p12** and click **OK**. This file is in **~/Downloads**.\n\n![](images/lab3-task2.16-download-p12.png)\n\n17.Back to **Cluster Connection**. Verify the Certificate password (it might be different for you).\n\n![](images/lab3-task2.17-truststore-password.png)\n\n**Keep this window open**.\n\n18.Open a terminal window, go to directory you created **~/eslab**. Open setdbparms.txt file and replace the parameters:\n\n```\n\n1.Enter over <securityIdentity> customerinfoapp from the Cluster Connection window, click copy icon from SCRAM username and paste over <SCRAM USER>.\n2.From the Cluster Connection window, click copy icon from SCRAM password and paste over <SCRAM PASSWORD>.\n3.From the Cluster Connection window, click copy icon from PKCS12 certificate and paste over <TRUSTSTORE PASSWORD>.\n4.Enter a name customer in <anyname> and click Save.\n\n```\n\n![](images/lab3-task2.18-a-cluster-connectio-parameters.png)\n\n![](images/lab3-task2.18-b-setfbparms.png)\n\n## Configuring App Connect Enterprise\n\n### Task 2 - Configuring App Connect Enterprise flow using App Connect Enterprise Toolkit\n\n\nBefore you start the lab, make sure that you have installed **IBM App Connect Enterprise toolkit**. Go to  this link https://www-01.ibm.com/marketing/iwm/mrs/DirectDownload?source=swg-wmbfd&lang=en_US and make the installation.\n\nYou have created a topic in Event Streams created. App Connect Enterprise produces a message and send it to the Event Streams topic. In this task, you configure an App Connect Enterprise message flow and generate a BAR file to deploy in the App Connect Enterprise Dashboard.\n\n1.Start IBM App Connect Enterprise. Create the App Connect Enterprise workspace directory as **ace-es** folder (**~/IBM/ACET11/workspace/ace-es**). Click **OK** to open App Connect Enterprise toolkit.\n\n![](images/lab3-task3.1-ace-workspace-ace-es.png)\n\n2.Import a new workspace. In the **Application Development** window, right mouse button and click **Import**.\n\n![](images/lab3-task3.2-new-import.png)\n\n3.Select in IBM Integration folder, **Project Interchange**. Click **Next**.\n\n![](images/lab3-task3.3-project-interchsnge.png)\n\n4.Locate in es-lab folder ace-es.zip file and click **Browse** and select **ace-es.zip** file to import.\n\n![](images/lab3-task3-4-select-ace-es.png)\n\n5.Verify that you imported the correct zip file to correct project. Click **Finish**.\n\n![](images/lab3-task3.5-import-ace-es.png)\n\n6.Before working on the message flow, create a policy. Click **New ->Start by creating a Policy project**. Policies can control particular node properties, such as connection credentials, and certain aspects of message flow behavior, including flow rate. Policies provide a shared and managed definition that you can reuse.\n\n![](images/lab3-task3.6-start-by-creating-policy.png)\n\n7.In the **Create a Policy project**, enter the policy name **customerpolicy** (this is the policy associated in Kafka Producer) and click **Finish**.\n\n![](images/lab3-task3.7-createapolicy.png)\n\n8.You created a policy project. Click **customerpolicy->(New..)**.\n\n![](images/lab3-task3.8-new-policy.png)\n\n9.In the pop-up window. Click **Policy** link.\n\n![](images/lab3-task3.9-new-artifact-policy.png)\n\n10.Enter **customer** as policy name and click **Finish** .\n\n![](images/lab3-task3.10-new-policy-policy-name.png)\n\n11.In the Policy enter or replace the policy configuration.\n\n```\n\n1.Change Type to Kafka.\n2.Automatically Template changes to Kafka.\n3.Enter the Bootstrap servers: es-demo-kafka-bootstrap-cp4i.mycluster-wdc06-c3c-32x64-4e85092308b6e4e8c206c47df210f622-0000.us-east.containers.appdomain.cloud:443 (You can copy from bootstrap.address file).\n4.Select SASL_SSL as Security Protocol.\n5.Enter SCRAM-SHA-512 as SASL Mechanism.\n6.Enter customerinfoapp as Security Identity (DSN).\n7.Set the SASL config property to org.apache.kafka.common.security.scram.ScramLoginModule required; (Important consider “ required;“ both are part of the value). This value specifies the SASL configuration to be used when connecting to the Kafka cluster.\n8.Delete default JKS as SSL keystore type.\n9.Enter /home/aceuser/truststores/es-cert.p12 as the SSL truststore location.\n10.In SSL truststore type, replace default JKS to PKCS12.\n11.Set truststorePass in SSL truststore security identity.\n12.Save the policy with CTRL+S.\n\n**Keep the rest as default**\n\n```\n![](images/lab3-task3.11-policy-creation.png)\n\n12.In Application Developer on the left bar, select **customerinfo -> Resources -> Subflows**.\n\n```\n1.Click getid.subflow. Some errors might appear , you fix this after you complete and save message flow.\n2.Select customerinfo node (Kafka Producer node)\n3.Click Properties.\n4.Select Basic properties\n5.Enter topic name: customerinfo (the topic name that you created in Event Streams).\n6.Paste the Bootstrap servers address: es-demo-kafka-bootstrap-cp4i.mycluster-wdc06-c3c-32x64-4e85092308b6e4e8c206c47df210f622-0000.us-east.containers.appdomain.cloud:443(the address is found in Event Streams, in Connect to this cluster->Cluster connection or you can edit ~/eslab/bootstrap.address file))\n\n```\n![](images/lab3-task3.12-kafka-producer-parameters.png)\n\n13.In the Security Tab, set the **Security Protocol** to **SASL SSL** and **SSL protocol** to **TLSv1.2**.\n\n![](images/lab3-task3.13-security-sasl.png)\n\n14.In **Properties**, select **Policy** and click **Browse** to assign a policy to the Kafka Node.\n\n![](images/lab3-task3.14-properties-policy-browse.png)\n\n15.Select the policy that you created and saved **{customerpolicy}:customer**.\n\n![](images/lab3-task3.15-select-policy.png)\n\n16.Save the message flow, click the Save button.\n\n![](images/lab3-task3.16-save-flow.png)\n\n17.You need to deploy the **customerinfo** application in App Connect Enterprise server. Select the **customerinfo application**. Click **File-> New-> BAR** file.\n\n![](images/lab3-task3.17-create-new-bar-file.png)\n\n18.Enter the suggested BAR file name: **customerinfo** and click **Finish** .\n\n![](images/lab3-task3.18-create-customerinfo-bar.png)\n\n19.Check **customerinfo** application box on the REST API tree. If necessary scroll right to check **Compile and in-line resource** and click **Build and Save**.\n\n![](images/lab3-task3.19-select-rest-api-compile.png)\n\n20.A pop-up window displays the message “Operation completed successfully.” Click **Ok** to confirm and close the App Connect Enterprise.\n\n21.Check **Policies** and **customerpolicy**. Click **Build and Save** and **OK** on Override Configurable Properties.\n\n![](images/lab3-task3.20-21.save-policy.png)\n\n22.You have a **BAR** file and **policy** document created in App Connect Enterprise workspace. Find **IBM ->ACET11->workspace->ace-es**. You see folders, locate **customerpolicy** folder and Compress (zip).\n\n![](images/lab3-task3.22-compress-policy.png)\n\n## Deploying App Connect Enterprise\n\n### Task 3 – Deploying App Connect BAR file on App Connect Enterprise Server\n\nThe App Connect Enterprise toolkit generated a BAR file. The BAR file has all information to run an App Connect Enterprise instance.\nThis release introduces a new Operator-based approach for packaging, deploying, and managing App Connect in a containerized environment. The IBM App Connect Enterprise certified container is now deployed to a Red Hat OpenShift cluster by using the new IBM App Connect Operator. This Operator is distributed through the IBM Entitled Registry, and can be installed from the Red Hat OpenShift OperatorHub. We’ve updated the look of our user interfaces to help improve the way you work.\n\n1.Open a browser and click **IBM Cloud Pak for Integration&& toolbar. Click **Capabilities** and select the **App Connect Dashboard** link.\n\n![](images/cloudpak-capabilities.png)\n\n2.If you might receive the login page, click **Log in**. (the username and password are saved).\n3.On the left menu, select **Configuration** icon.\n\n![](images/lab3-task4.3-ace-configuration.png)\n\n4.Click **Create configuration**.\n\n![](images/lab3-task4.4-create-configuration.png)\n\n5.Click the arrow and select **Policy Project**. In the **Import compressed policy project**, move the mouse and click **Drag and drop a file or click to upload**.\n\n![](images/lab3-task4.5-policy-project.png)\n\n6.Open **~/IBM/ACET11/workspace/ace-es** and select **customerpolicy.zip** and click **Open**. To upload to App Connect Dashboard and click **Create**.\n\n![](images/lab3-task4.6-open-customerpolicy.png)\n\n7.Check if the **customerpolicy.zip** is uploaded and Click **Create**.\n\n![](images/lab3-task4.7-create-configuration-policy.png)\n\n8.Upload the **sertdbparms.txt**. Click **Create configuration**.\n\n![](images/lab3-task4.8-create-cofiguration-setdbparms.png)\n\n9.Click on the Import **setdbparams.txt** icon.\n\n![](images/lab3-task4.9-upload-setdbparms.png)\n\n10.Click on the Import s**etdbparams.txt** icon and click **Drag and drop a setdbparms.txt file or click to upload**.\n\n![](images/lab3-task4.10-drag-drop-setdbparms.png)\n\n11.In the **file upload**, open **~/eslab**, select **setdbparms.txt** and click **Open**.\n\n![](images/lab3-task4.11-open-setdbparms.png)\n\n12.Verify the setdbparms is loaded and click **Create**.\n\n![](images/lab3-task4.12-confirm-setdbparms.png)\n\n13.The same process, upload the **truststore** certificate. Click **Create configuration**.\n\n![](images/lab3-task4.13-create-configuration-truststore.png)\n\n14.Select **Truststore** and click **Drag and drop a truststore file to upload**.\n\n![](images/lab3-task4.14-drag-drop-truststore.png)\n\n15.In **File Upload**, go to **~/Downloads**, select **es-cert.p12** file and click **Open**.\n\n16.Click **Create** to upload the file to App Connect Dashboard.\n\n![](images/lab3-task4.16-create-truststore-configuration.png)\n\n17.You have uploaded three configuration files. Now, you upload and configure the BAR file in App Connect Dashboard. Click the **Home** icon on the left.\n\n![](images/lab3-task4.17-home-ace-dashboard.png)\n\n18.In the **Welcome to IBM App Connect** page. Click **Create a server**.\n\n![](images/lab3-task4.18-create-server-ace.png)\n\n19.In this version, you two ways to create a BAR file: using App Connect Toolkit or App Connect Designer (App Connect Designer provides an authoring environment in which you can create, test, and share flows for an API. You can share your flows by using the export and import functions, or by adding them to an Asset Repository for reuse). Select **Toolkit integration** and click **Next**.\n\n![](images/lab3-task4.19-toolkit-integration.png)\n\n20.Upload the BAR file, clicking **Drag and drop a BAR file or click to upload**.\n\n![](images/lab3-task4.20-drag-bar-file.png)\n\n21.In File Upload, go to **~/IBM/ACET11/workspaces/ace-es/BARfiles**, select **customerinfo.bar**, click **Open**, and click **Next**.\n\n![](images/lab3-task4.21-select-customerinfo.png)\n\n22.You have uploaded the configuration files, check all three files (**customerpolicy.zip, setdbparms.txt and es-cert.p12**) and click **Next**.\n\n![](images/lab3-task4.22-check-configuration.png)\n\n23.Configure the Integration Server.\n\n```\n1.Enter the server name: customerinfo .\n2.Switch to On Enable Operations Dashboard\n3.Enter cp4i as Operations Dashboard namespace.\n\n```\n![](images/lab3-task4.23-create-ace-customerinfo.png)\n\n## Testing App Connect Enterprise\n\n### Task 4  – Testing App Connect Enterprise API sending a message to Event Streams\n\nYou verify if the message you created in App Connect Dashboard (Integration server) arrived in Event Streams topics.\n\n1.App Connect Dashboard will create **customerinfo server**. After 5 minutes, refresh the Browser and see the server is **Started**. Click **customerinfo icon**.\n\n![](images/lab3-task5.1-customerinfo-started.png)\n\n2.Click the **customerinfo API** icon.\n\n![](images/lab3-task5.2-customerinfo-api.png)\n\n3.On the Documentation tab, the overview section displays the type of API and the base URL for the API endpoint. A **Download Open API Document** link is also provided for the **OpenAPI** document that describes the API. If downloaded, the document is saved as a YAML file to the default download location that is configured for your browser. Copy the Endpoint.\n\n![](images/lab3-task5.3-copy-address-customerinfo.png)\n\n4.Open a terminal window, enter the **CURL** command: “ curl --request GET --url http://customerinfo-http-cp4i.mycluster-wdc06-c3c-32x64-4e85092308b6e4e8c206c47df210f622-0000.useast.containers.appdomain.cloud:80/customerinfo/v1/00000 “. Paste the Endpoint and complete after v1/**00000**. Check the results.\n\n![](images/lab3-task5.4-curl-api-customerinfo.png)\n\n5.A message was sent from App Connect (Integration Server) to IBM Event Streams. Go to **IBM Cloud Pak for Integration**.  Select **Runtimes** and click **Event Streams** application.\n\n![](images/lab3-task2.4-runtimes-eventstreams.png)\n\n6.In Welcome to IBM Event Streams page, Click Topics on the left, to open the topics list of this Event Streams instance.\n\n![](images/lab3-task5.6-topics.png)\n\n7.In the Topics page, click the topic **customerinfo** to open the topic page.\n\n![](images/lab3-task5.7-customerinfo.png)\n\n8.Click **Messages** to check if the message from App Connect Enterprise has arrived. You see the list of messages that are stored on the Event Streams topic. Take time to look at the monitor to explore the information.\n\n![](images/lab3-task5.8-messages.png)\n\n9.Click the message and Verify the message on the customerinfo topic. Verify the Customerid:**0000**.\n\n![](images/lab3-task5.9-select-message.png)\n\n10.Click **Monitor** icon on the left to look at Event Streams monitor.\n\n![](images/lab3-task5.10-event-streams-monitoring.png)\n\n11.You can analyzing the incoming and outcoming messages, per period (hour, day, week, and month).\n\n![](images/lab3-task5.11-eventstreams-monitoring.png)\n\n## Operations Dashboard (tracing)\n\n### Task 5 - Using Operations Dashboard (tracing)\n\nThe Operations Dashboard collects data from all the registered capabilities (such as MQ) in real time. By default, and for this lab, 10 percent of traffic is sampled.\n\nIBM Cloud Pak for Integration Operations Dashboard has adopted OpenTracing API specification for collecting tracing data. OpenTracing is comprised of an API specification for distributed tracing, frameworks and libraries that have implemented the specification and documentation.\n\no\tTrace: The description of a transaction as it moves through IBM Cloud Pak for Integration platform.\no\tSpan: A named, timed operation representing a piece of the workflow (e.g. calling an API, invoking a message flow or placing a message in a queue or a topic).\no\tSpan context: Trace information that accompanies the distributed transaction, including when it passes the service to service over the network or through a message bus.\n\n1.Go to the **IBM Cloud Pak for Integration**. Click the **tracing** application  to open the Operations Dashboard instance.\n\n![](images/lab3-task6.1-welcome-tracing.png)\n\n2.In the Tracing page, check the Overview page. You see all the products that you can use this tool: **APIC (including DataPower), APP Connect and MQ**. You see all the tracing of MQ, App Connect and APIC (You see how to configure tracing in APIC lab). Operations Dashboard Add-on is based on Jaeger open source project and the OpenTracing standard to monitor and troubleshoot microservices-based distributed systems. Operations Dashboard can distinguish call paths and latencies. DevOps personnel, developers, and performance engineers now have one tool to visualize throughput and latency across integration components that run on Cloud Pak for Integration. Cloud Pak for Integration - Operations Dashboard Add-on is designed to help organizations that need to meet and ensure maximum service availability and react quickly to any variations in their systems.\n\n![](images/lab3-task6.2-tracing-overview.png)\n\n3.You can monitor each product separately. Click **App C overview**.\n\n![](images/lab3-task6.3-Appc-overview.png)\n\n4.In the tracing page, select **traces** the menu on the left.\n\n![](images/lab3-task6.4-traces-tracing.png)\n\n5.\tYou see the list of tracing, select the **customerinfo** line to analyze the trace of the application **customerinfo**.\n\n![](images/lab3-task6.5-tracing.png)\n\n\n## Summary\n\n\nYou have successfully completed this lab. In this lab you learned how to:\n\n•\tCreate a topic in Kafka.\n•\tCreate an integration between an API service and Kafka\n•\tDeploy the new integration as containers in Kubernetes.\n•\tUse Operations Dashboard tool\n\nNow that you’ve created a topic in Kafka (Event Streams), applications are able to subscribe and received data. To try out more labs, go to Cloud Pak for Integration Demos. For more information about the Cloud Pak for Integration, go to https://www.ibm.com/cloud/cloud-pak-for-integration.\n","type":"Mdx","contentDigest":"c46e900790d4485d5720104957df2148","counter":1522,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Augment existing business functions with new applications using Kafka"},"exports":{},"rawBody":"---\ntitle: Augment existing business functions with new applications using Kafka\n---\n\nimport ArticleDetails from '../../../../src/gatsby-theme-carbon/components/ArticleDetails'\n\n<ArticleDetails name=\"Carlos Hirata, Ravi Katikala\" lastUpdated=\"January 2021\" readTimeMinutes=\"60\" />\n\nIn this tutorial, you use the Cloud Pak for Integration 2020.4.1 to create an App Integration flow (IBM App Connect Enterprise) that sends messages to a queue (IBM MQ) on OpenShift 4.6.\n\n<AnchorLinks>\n  <AnchorLink>Event Streams Topic</AnchorLink>\n  <AnchorLink>Configuring App Connect Enterprise</AnchorLink>\n  <AnchorLink>Deploying App Connect Enterprise</AnchorLink>\n  <AnchorLink>Testing App Connect Enterprise</AnchorLink>\n  <AnchorLink>Operations Dashboard (tracing)</AnchorLink>\n  <AnchorLink>Summary</AnchorLink>\n</AnchorLinks>\n\n### Introduction\n\nAugment existing business functions with new applications using Kafka.\n\nThe most interesting and impactful new applications in an enterprise are those that provide interactive experiences by reacting to existing systems carrying out a business function. In this tutorial, we take a look at an example from the retail industry. Starting with an existing API orchestrating the business function to \"place an order\". Let's say that when a customer places an order, we want to provide a real-time response. We want to reward the customer with points in a customer loyalty app or gamification experience or sign them up for a certain email nurture program. To do that, we need each order to emit an event. The Cloud Pak for Integration combines integration capabilities with Kafka based event streaming to make the data available for cloud-native applications to subscribe to a Kafka topic and use it for various business purposes.\n\nIn this tutorial, you create a topic in Kafka, modify an integration flow to call an API, emit an event onto the topic, and use the tracing tool to verify the message from App Connect Enterprise to Event Streams.\nThe Cloud Pak for Integration - Operations Dashboard Add-on provides cross-component monitoring and tracing capabilities that enable IT personnel to quickly analyze performance bottlenecks and latency problems across integration components to help ensure high levels of service.\n\n### Takeaways\n\n- Creating and Configuring an Event Streams Topic\n- Configure App Connect Enterprise message flow using App Connect Enterprise toolkit\n- Configuring App Connect Enterprise service\n- Deploying App Connect BAR file on App Connect Enterprise Server\n- Testing App Connect Enterprise API sending a message to Event Streams\n- Checking this message using Operations Dashboard (tracing).\n\n\n\n## Event Streams Topic\n\n### Task 1 – Creating and Configuring an Event Streams Topic\n\nCreating an Event Streams topic in the existing Event Streams instance.\n\n1.Before you start the lab. Download **eslab.zip** from **Github**, open a browser and enter: https://github.com/ibm-cloudintegration/dte-labs/tree/master/Lab3-RespondtoEvents . Click **eslab.zip** .\n\n![](images/lab3-task2.1-eslab.png)\n\n2.Click **Download**. The file **eslab.zip** is in your  **~/Download** directory.\n\n![](images/lab3-task2.2-download-eslab.png)\n\n3.Execute **unzip eslab.zip** will create the work directory **~/eslab** with 3 files: **ace-es.zip** (App Connect Enterprise project), **setdbparms.txt** (App Connect configuration file) and **bootstrap.address** (you can use this file to store Event Streams Bootstrap Address).\n\n![](images/lab3-task2.3-unzip-eslab.png)\n\n4.In **Runtimes** click **event-streams** link (in the kafka cluster instance type).\n\n![](images/lab3-task2.4-runtimes-eventstreams.png)\n\n5.In the Welcome to IBM Event Streams page. Click **System is healthy**  and check the status of the instance.\n\n![](images/lab3-task2.5-access-system-healthy.png)\n\n6.Verify if all Event Streams components are running and ready.\n\n![](images/lab3-task2.6-systemishealthy.png)\n\n7.Click **Create a topic** to configure topic.\n\n![](images/lab3-task2.7-createatopic.png)\n\n8.View the full range of configuration options by setting the Show all available options to on. Create the topic as follows:\n\n```\n1.In Show all available options, change to On.\n2.Topic name: customerinfo.\n3.Click Create topic.\n\n```\n\n![](images/lab3-task2.8-createatopic-parameters.png)\n\n9.The topic is created, click to **connect to this cluster**. By default Event Streams requires clients to be authorized to write to topics. The available authentication mechanisms for use with the REST Producer are MutualTLS (tls) and SCRAM SHA 512 (scram-sha-512). For more information about these authentication mechanisms (https://en.wikipedia.org/wiki/Salted_Challenge_Response_Authentication_Mechanism).\n\n![](images/lab3-task2.9-connecttothiscluster.png)\n\n10.To connect an application or tool to this cluster, you need the address of a Kafka listener, a credential to authenticate with, and a certificate depending on the listener.\nFor this lab, use an External listener. App Connect needs securities parameters to access Event Streams, the truststore password, certificate and SCRAM user ID and password.\nSave the bootstrap server **External (1)**, clicking the **copy icon**. You can use **bootstrap.address** file.\nFor this lab you use external listener to verify how to access Event Streams as external application.\n\n![](images/lab3-task2.10-savebootstrap.png)\n\n11.Click **Generate SCRAM credentials** .\n\n![](images/lab3-task2.11-generatescram.png)\n\n12.The Generate credentials for your application window you create credential name and how your application works with Event Streams. Credential name, it is your kafkauser. You can see this in the OpenShift console.\n\n```\n1.\tEnter the credential name: customerinfoapp.\n2.\tCheck Produce and consume messages, and read schemas.\n3.\tClick Next.\n\n```\n![](images/lab3-task2.12-generate-credentials.png)\n\n13.In the window, Which topics does the application need to access?, you can specify which the topics, the application accesses.\n\n```\n1.\tCheck All topics.\n2.\tClick Next.\n\n```\n![](images/lab3-task2.13-which-consumer-access.png)\n\n14.In the window, **Which consumer group does the application need to access?**, a consumer group is a group of consumers cooperating to consume messages from one or more topics. The consumers in a group all use the same value for the group ID configuration.\n\n```\n1.\tCheck All consumer groups.\n2.\tClick Next.\n\n```\n\n![](images/lab3-task2.14-which-topics-need-access.png)\n\n15.In the window, **Choose which transactional IDs the application can access?**, you can control the ability to use the transaction capability in Kafka.\n\n```\n1.Keep No transactional IDs checked.\n2.Click Generate credentials.\n\n```\n![](images/lab3-task2.15-which-transactional-id.png)\n\n16.Back to **Cluster connection** window. Verify the bootstrap server as an external listener, the username and password in **Generated Scram Credentials**. Scroll to **Certificates** and click **Download certificate**. Save **es-cert.p12** and click **OK**. This file is in **~/Downloads**.\n\n![](images/lab3-task2.16-download-p12.png)\n\n17.Back to **Cluster Connection**. Verify the Certificate password (it might be different for you).\n\n![](images/lab3-task2.17-truststore-password.png)\n\n**Keep this window open**.\n\n18.Open a terminal window, go to directory you created **~/eslab**. Open setdbparms.txt file and replace the parameters:\n\n```\n\n1.Enter over <securityIdentity> customerinfoapp from the Cluster Connection window, click copy icon from SCRAM username and paste over <SCRAM USER>.\n2.From the Cluster Connection window, click copy icon from SCRAM password and paste over <SCRAM PASSWORD>.\n3.From the Cluster Connection window, click copy icon from PKCS12 certificate and paste over <TRUSTSTORE PASSWORD>.\n4.Enter a name customer in <anyname> and click Save.\n\n```\n\n![](images/lab3-task2.18-a-cluster-connectio-parameters.png)\n\n![](images/lab3-task2.18-b-setfbparms.png)\n\n## Configuring App Connect Enterprise\n\n### Task 2 - Configuring App Connect Enterprise flow using App Connect Enterprise Toolkit\n\n\nBefore you start the lab, make sure that you have installed **IBM App Connect Enterprise toolkit**. Go to  this link https://www-01.ibm.com/marketing/iwm/mrs/DirectDownload?source=swg-wmbfd&lang=en_US and make the installation.\n\nYou have created a topic in Event Streams created. App Connect Enterprise produces a message and send it to the Event Streams topic. In this task, you configure an App Connect Enterprise message flow and generate a BAR file to deploy in the App Connect Enterprise Dashboard.\n\n1.Start IBM App Connect Enterprise. Create the App Connect Enterprise workspace directory as **ace-es** folder (**~/IBM/ACET11/workspace/ace-es**). Click **OK** to open App Connect Enterprise toolkit.\n\n![](images/lab3-task3.1-ace-workspace-ace-es.png)\n\n2.Import a new workspace. In the **Application Development** window, right mouse button and click **Import**.\n\n![](images/lab3-task3.2-new-import.png)\n\n3.Select in IBM Integration folder, **Project Interchange**. Click **Next**.\n\n![](images/lab3-task3.3-project-interchsnge.png)\n\n4.Locate in es-lab folder ace-es.zip file and click **Browse** and select **ace-es.zip** file to import.\n\n![](images/lab3-task3-4-select-ace-es.png)\n\n5.Verify that you imported the correct zip file to correct project. Click **Finish**.\n\n![](images/lab3-task3.5-import-ace-es.png)\n\n6.Before working on the message flow, create a policy. Click **New ->Start by creating a Policy project**. Policies can control particular node properties, such as connection credentials, and certain aspects of message flow behavior, including flow rate. Policies provide a shared and managed definition that you can reuse.\n\n![](images/lab3-task3.6-start-by-creating-policy.png)\n\n7.In the **Create a Policy project**, enter the policy name **customerpolicy** (this is the policy associated in Kafka Producer) and click **Finish**.\n\n![](images/lab3-task3.7-createapolicy.png)\n\n8.You created a policy project. Click **customerpolicy->(New..)**.\n\n![](images/lab3-task3.8-new-policy.png)\n\n9.In the pop-up window. Click **Policy** link.\n\n![](images/lab3-task3.9-new-artifact-policy.png)\n\n10.Enter **customer** as policy name and click **Finish** .\n\n![](images/lab3-task3.10-new-policy-policy-name.png)\n\n11.In the Policy enter or replace the policy configuration.\n\n```\n\n1.Change Type to Kafka.\n2.Automatically Template changes to Kafka.\n3.Enter the Bootstrap servers: es-demo-kafka-bootstrap-cp4i.mycluster-wdc06-c3c-32x64-4e85092308b6e4e8c206c47df210f622-0000.us-east.containers.appdomain.cloud:443 (You can copy from bootstrap.address file).\n4.Select SASL_SSL as Security Protocol.\n5.Enter SCRAM-SHA-512 as SASL Mechanism.\n6.Enter customerinfoapp as Security Identity (DSN).\n7.Set the SASL config property to org.apache.kafka.common.security.scram.ScramLoginModule required; (Important consider “ required;“ both are part of the value). This value specifies the SASL configuration to be used when connecting to the Kafka cluster.\n8.Delete default JKS as SSL keystore type.\n9.Enter /home/aceuser/truststores/es-cert.p12 as the SSL truststore location.\n10.In SSL truststore type, replace default JKS to PKCS12.\n11.Set truststorePass in SSL truststore security identity.\n12.Save the policy with CTRL+S.\n\n**Keep the rest as default**\n\n```\n![](images/lab3-task3.11-policy-creation.png)\n\n12.In Application Developer on the left bar, select **customerinfo -> Resources -> Subflows**.\n\n```\n1.Click getid.subflow. Some errors might appear , you fix this after you complete and save message flow.\n2.Select customerinfo node (Kafka Producer node)\n3.Click Properties.\n4.Select Basic properties\n5.Enter topic name: customerinfo (the topic name that you created in Event Streams).\n6.Paste the Bootstrap servers address: es-demo-kafka-bootstrap-cp4i.mycluster-wdc06-c3c-32x64-4e85092308b6e4e8c206c47df210f622-0000.us-east.containers.appdomain.cloud:443(the address is found in Event Streams, in Connect to this cluster->Cluster connection or you can edit ~/eslab/bootstrap.address file))\n\n```\n![](images/lab3-task3.12-kafka-producer-parameters.png)\n\n13.In the Security Tab, set the **Security Protocol** to **SASL SSL** and **SSL protocol** to **TLSv1.2**.\n\n![](images/lab3-task3.13-security-sasl.png)\n\n14.In **Properties**, select **Policy** and click **Browse** to assign a policy to the Kafka Node.\n\n![](images/lab3-task3.14-properties-policy-browse.png)\n\n15.Select the policy that you created and saved **{customerpolicy}:customer**.\n\n![](images/lab3-task3.15-select-policy.png)\n\n16.Save the message flow, click the Save button.\n\n![](images/lab3-task3.16-save-flow.png)\n\n17.You need to deploy the **customerinfo** application in App Connect Enterprise server. Select the **customerinfo application**. Click **File-> New-> BAR** file.\n\n![](images/lab3-task3.17-create-new-bar-file.png)\n\n18.Enter the suggested BAR file name: **customerinfo** and click **Finish** .\n\n![](images/lab3-task3.18-create-customerinfo-bar.png)\n\n19.Check **customerinfo** application box on the REST API tree. If necessary scroll right to check **Compile and in-line resource** and click **Build and Save**.\n\n![](images/lab3-task3.19-select-rest-api-compile.png)\n\n20.A pop-up window displays the message “Operation completed successfully.” Click **Ok** to confirm and close the App Connect Enterprise.\n\n21.Check **Policies** and **customerpolicy**. Click **Build and Save** and **OK** on Override Configurable Properties.\n\n![](images/lab3-task3.20-21.save-policy.png)\n\n22.You have a **BAR** file and **policy** document created in App Connect Enterprise workspace. Find **IBM ->ACET11->workspace->ace-es**. You see folders, locate **customerpolicy** folder and Compress (zip).\n\n![](images/lab3-task3.22-compress-policy.png)\n\n## Deploying App Connect Enterprise\n\n### Task 3 – Deploying App Connect BAR file on App Connect Enterprise Server\n\nThe App Connect Enterprise toolkit generated a BAR file. The BAR file has all information to run an App Connect Enterprise instance.\nThis release introduces a new Operator-based approach for packaging, deploying, and managing App Connect in a containerized environment. The IBM App Connect Enterprise certified container is now deployed to a Red Hat OpenShift cluster by using the new IBM App Connect Operator. This Operator is distributed through the IBM Entitled Registry, and can be installed from the Red Hat OpenShift OperatorHub. We’ve updated the look of our user interfaces to help improve the way you work.\n\n1.Open a browser and click **IBM Cloud Pak for Integration&& toolbar. Click **Capabilities** and select the **App Connect Dashboard** link.\n\n![](images/cloudpak-capabilities.png)\n\n2.If you might receive the login page, click **Log in**. (the username and password are saved).\n3.On the left menu, select **Configuration** icon.\n\n![](images/lab3-task4.3-ace-configuration.png)\n\n4.Click **Create configuration**.\n\n![](images/lab3-task4.4-create-configuration.png)\n\n5.Click the arrow and select **Policy Project**. In the **Import compressed policy project**, move the mouse and click **Drag and drop a file or click to upload**.\n\n![](images/lab3-task4.5-policy-project.png)\n\n6.Open **~/IBM/ACET11/workspace/ace-es** and select **customerpolicy.zip** and click **Open**. To upload to App Connect Dashboard and click **Create**.\n\n![](images/lab3-task4.6-open-customerpolicy.png)\n\n7.Check if the **customerpolicy.zip** is uploaded and Click **Create**.\n\n![](images/lab3-task4.7-create-configuration-policy.png)\n\n8.Upload the **sertdbparms.txt**. Click **Create configuration**.\n\n![](images/lab3-task4.8-create-cofiguration-setdbparms.png)\n\n9.Click on the Import **setdbparams.txt** icon.\n\n![](images/lab3-task4.9-upload-setdbparms.png)\n\n10.Click on the Import s**etdbparams.txt** icon and click **Drag and drop a setdbparms.txt file or click to upload**.\n\n![](images/lab3-task4.10-drag-drop-setdbparms.png)\n\n11.In the **file upload**, open **~/eslab**, select **setdbparms.txt** and click **Open**.\n\n![](images/lab3-task4.11-open-setdbparms.png)\n\n12.Verify the setdbparms is loaded and click **Create**.\n\n![](images/lab3-task4.12-confirm-setdbparms.png)\n\n13.The same process, upload the **truststore** certificate. Click **Create configuration**.\n\n![](images/lab3-task4.13-create-configuration-truststore.png)\n\n14.Select **Truststore** and click **Drag and drop a truststore file to upload**.\n\n![](images/lab3-task4.14-drag-drop-truststore.png)\n\n15.In **File Upload**, go to **~/Downloads**, select **es-cert.p12** file and click **Open**.\n\n16.Click **Create** to upload the file to App Connect Dashboard.\n\n![](images/lab3-task4.16-create-truststore-configuration.png)\n\n17.You have uploaded three configuration files. Now, you upload and configure the BAR file in App Connect Dashboard. Click the **Home** icon on the left.\n\n![](images/lab3-task4.17-home-ace-dashboard.png)\n\n18.In the **Welcome to IBM App Connect** page. Click **Create a server**.\n\n![](images/lab3-task4.18-create-server-ace.png)\n\n19.In this version, you two ways to create a BAR file: using App Connect Toolkit or App Connect Designer (App Connect Designer provides an authoring environment in which you can create, test, and share flows for an API. You can share your flows by using the export and import functions, or by adding them to an Asset Repository for reuse). Select **Toolkit integration** and click **Next**.\n\n![](images/lab3-task4.19-toolkit-integration.png)\n\n20.Upload the BAR file, clicking **Drag and drop a BAR file or click to upload**.\n\n![](images/lab3-task4.20-drag-bar-file.png)\n\n21.In File Upload, go to **~/IBM/ACET11/workspaces/ace-es/BARfiles**, select **customerinfo.bar**, click **Open**, and click **Next**.\n\n![](images/lab3-task4.21-select-customerinfo.png)\n\n22.You have uploaded the configuration files, check all three files (**customerpolicy.zip, setdbparms.txt and es-cert.p12**) and click **Next**.\n\n![](images/lab3-task4.22-check-configuration.png)\n\n23.Configure the Integration Server.\n\n```\n1.Enter the server name: customerinfo .\n2.Switch to On Enable Operations Dashboard\n3.Enter cp4i as Operations Dashboard namespace.\n\n```\n![](images/lab3-task4.23-create-ace-customerinfo.png)\n\n## Testing App Connect Enterprise\n\n### Task 4  – Testing App Connect Enterprise API sending a message to Event Streams\n\nYou verify if the message you created in App Connect Dashboard (Integration server) arrived in Event Streams topics.\n\n1.App Connect Dashboard will create **customerinfo server**. After 5 minutes, refresh the Browser and see the server is **Started**. Click **customerinfo icon**.\n\n![](images/lab3-task5.1-customerinfo-started.png)\n\n2.Click the **customerinfo API** icon.\n\n![](images/lab3-task5.2-customerinfo-api.png)\n\n3.On the Documentation tab, the overview section displays the type of API and the base URL for the API endpoint. A **Download Open API Document** link is also provided for the **OpenAPI** document that describes the API. If downloaded, the document is saved as a YAML file to the default download location that is configured for your browser. Copy the Endpoint.\n\n![](images/lab3-task5.3-copy-address-customerinfo.png)\n\n4.Open a terminal window, enter the **CURL** command: “ curl --request GET --url http://customerinfo-http-cp4i.mycluster-wdc06-c3c-32x64-4e85092308b6e4e8c206c47df210f622-0000.useast.containers.appdomain.cloud:80/customerinfo/v1/00000 “. Paste the Endpoint and complete after v1/**00000**. Check the results.\n\n![](images/lab3-task5.4-curl-api-customerinfo.png)\n\n5.A message was sent from App Connect (Integration Server) to IBM Event Streams. Go to **IBM Cloud Pak for Integration**.  Select **Runtimes** and click **Event Streams** application.\n\n![](images/lab3-task2.4-runtimes-eventstreams.png)\n\n6.In Welcome to IBM Event Streams page, Click Topics on the left, to open the topics list of this Event Streams instance.\n\n![](images/lab3-task5.6-topics.png)\n\n7.In the Topics page, click the topic **customerinfo** to open the topic page.\n\n![](images/lab3-task5.7-customerinfo.png)\n\n8.Click **Messages** to check if the message from App Connect Enterprise has arrived. You see the list of messages that are stored on the Event Streams topic. Take time to look at the monitor to explore the information.\n\n![](images/lab3-task5.8-messages.png)\n\n9.Click the message and Verify the message on the customerinfo topic. Verify the Customerid:**0000**.\n\n![](images/lab3-task5.9-select-message.png)\n\n10.Click **Monitor** icon on the left to look at Event Streams monitor.\n\n![](images/lab3-task5.10-event-streams-monitoring.png)\n\n11.You can analyzing the incoming and outcoming messages, per period (hour, day, week, and month).\n\n![](images/lab3-task5.11-eventstreams-monitoring.png)\n\n## Operations Dashboard (tracing)\n\n### Task 5 - Using Operations Dashboard (tracing)\n\nThe Operations Dashboard collects data from all the registered capabilities (such as MQ) in real time. By default, and for this lab, 10 percent of traffic is sampled.\n\nIBM Cloud Pak for Integration Operations Dashboard has adopted OpenTracing API specification for collecting tracing data. OpenTracing is comprised of an API specification for distributed tracing, frameworks and libraries that have implemented the specification and documentation.\n\no\tTrace: The description of a transaction as it moves through IBM Cloud Pak for Integration platform.\no\tSpan: A named, timed operation representing a piece of the workflow (e.g. calling an API, invoking a message flow or placing a message in a queue or a topic).\no\tSpan context: Trace information that accompanies the distributed transaction, including when it passes the service to service over the network or through a message bus.\n\n1.Go to the **IBM Cloud Pak for Integration**. Click the **tracing** application  to open the Operations Dashboard instance.\n\n![](images/lab3-task6.1-welcome-tracing.png)\n\n2.In the Tracing page, check the Overview page. You see all the products that you can use this tool: **APIC (including DataPower), APP Connect and MQ**. You see all the tracing of MQ, App Connect and APIC (You see how to configure tracing in APIC lab). Operations Dashboard Add-on is based on Jaeger open source project and the OpenTracing standard to monitor and troubleshoot microservices-based distributed systems. Operations Dashboard can distinguish call paths and latencies. DevOps personnel, developers, and performance engineers now have one tool to visualize throughput and latency across integration components that run on Cloud Pak for Integration. Cloud Pak for Integration - Operations Dashboard Add-on is designed to help organizations that need to meet and ensure maximum service availability and react quickly to any variations in their systems.\n\n![](images/lab3-task6.2-tracing-overview.png)\n\n3.You can monitor each product separately. Click **App C overview**.\n\n![](images/lab3-task6.3-Appc-overview.png)\n\n4.In the tracing page, select **traces** the menu on the left.\n\n![](images/lab3-task6.4-traces-tracing.png)\n\n5.\tYou see the list of tracing, select the **customerinfo** line to analyze the trace of the application **customerinfo**.\n\n![](images/lab3-task6.5-tracing.png)\n\n\n## Summary\n\n\nYou have successfully completed this lab. In this lab you learned how to:\n\n•\tCreate a topic in Kafka.\n•\tCreate an integration between an API service and Kafka\n•\tDeploy the new integration as containers in Kubernetes.\n•\tUse Operations Dashboard tool\n\nNow that you’ve created a topic in Kafka (Event Streams), applications are able to subscribe and received data. To try out more labs, go to Cloud Pak for Integration Demos. For more information about the Cloud Pak for Integration, go to https://www.ibm.com/cloud/cloud-pak-for-integration.\n","fileAbsolutePath":"/Users/rafaelosorio/Documents/GitHub/cp4i-demohub/src/pages/tutorials/CreateEngagingExperiences-old/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}